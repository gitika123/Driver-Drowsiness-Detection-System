{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VAiAyFvh64jC"
      },
      "source": [
        "# Driver Drowsiness Detection - Multimodal Transformer Model\n",
        "## Novel Fusion Architecture for Enhanced Performance\n",
        "\n",
        "This notebook implements a multimodal transformer-based driver drowsiness detection system featuring:\n",
        "- Vision Transformer (ViT) for spatial feature extraction\n",
        "- Temporal sequence modeling for video frame sequences  \n",
        "- Multi-modal fusion mechanisms (RGB, attention maps, temporal features)\n",
        "- Complete EDA and data science lifecycle\n",
        "- Interactive dashboard with real-time predictions\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xhyH3ymu64jE"
      },
      "source": [
        "## 1. Installation and Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OLPMDL2M64jF"
      },
      "outputs": [],
      "source": [
        "# Install required packages (with Python 3.12 compatible PyTorch)\n",
        "# Fix for Python 3.12 compatibility: Use PyTorch 2.1.0+ from official wheel\n",
        "# For Google Colab with GPU, use CUDA 12.1; for CPU or local, adjust as needed\n",
        "\n",
        "# Install PyTorch (Python 3.12 compatible)\n",
        "%pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121 -q\n",
        "\n",
        "# Install other packages\n",
        "%pip install transformers matplotlib scikit-learn gradio pandas seaborn pillow timm -q\n",
        "\n",
        "print(\"‚úì All packages installed successfully!\")\n",
        "print(\"Note: If CUDA 12.1 fails, try: %pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118 -q\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pb_LPPdY64jG"
      },
      "outputs": [],
      "source": [
        "# Setup paths\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    BASE_DIR = '/content/drive/MyDrive/cs163_ds'\n",
        "except:\n",
        "    BASE_DIR = '/Users/spartan/Downloads/cs163 Modules/project/cs163_ds'\n",
        "\n",
        "print(f\"Dataset directory: {BASE_DIR}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_dlqaAY164jG"
      },
      "source": [
        "## 2. Import Libraries and Data Loading\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2XL2pJva64jG"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from PIL import Image\n",
        "from collections import Counter\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import ViTModel, ViTImageProcessor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wQZDGga-64jH"
      },
      "source": [
        "## 3. Exploratory Data Analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YUCDLz1M64jI"
      },
      "outputs": [],
      "source": [
        "# Dataset exploration\n",
        "folders = [f for f in os.listdir(BASE_DIR) if os.path.isdir(os.path.join(BASE_DIR, f))]\n",
        "dataset_info = {}\n",
        "total_images = 0\n",
        "\n",
        "for folder in folders:\n",
        "    folder_path = os.path.join(BASE_DIR, folder)\n",
        "    images = [f for f in os.listdir(folder_path)\n",
        "             if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
        "    dataset_info[folder] = len(images)\n",
        "    total_images += len(images)\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"DATASET EXPLORATION\")\n",
        "print(\"=\"*60)\n",
        "for folder, count in dataset_info.items():\n",
        "    print(f\"{folder:20s}: {count:5d} images\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Total Images: {total_images}\")\n",
        "\n",
        "# Visualize class distribution\n",
        "plt.figure(figsize=(14, 6))\n",
        "plt.bar(dataset_info.keys(), dataset_info.values(), color=sns.color_palette(\"husl\", len(folders)))\n",
        "plt.xticks(rotation=45)\n",
        "plt.title(\"Class Distribution\", fontsize=14, fontweight='bold')\n",
        "plt.ylabel(\"Number of Images\")\n",
        "plt.tight_layout()\n",
        "plt.savefig('class_distribution_transformer.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6m_oH-JA64jJ"
      },
      "source": [
        "## 4. Multimodal Transformer Architecture\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DRkX6AAN64jJ"
      },
      "source": [
        "### üõ°Ô∏è Anti-Overfitting Measures Implemented:\n",
        "\n",
        "1. **Layer Freezing**: First 6 ViT encoder layers are frozen (only fine-tuning last layers)\n",
        "2. **Increased Dropout**:\n",
        "   - Fusion layer: 0.3 (was 0.1)\n",
        "   - Classifier: 0.5 and 0.4 (was 0.3)\n",
        "3. **Label Smoothing**: 0.1 smoothing factor in loss function\n",
        "4. **Lower Learning Rate**: 1e-5 (reduced from 2e-5)\n",
        "5. **Higher Weight Decay**: 0.05 (increased from 0.01)\n",
        "6. **Gradient Clipping**: max_norm=1.0 to prevent gradient explosion\n",
        "7. **Early Stopping**: Patience of 5 epochs with automatic best model restoration\n",
        "8. **Learning Rate Scheduling**: Adaptive reduction on plateau\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2xtNufOr64jK"
      },
      "outputs": [],
      "source": [
        "# Novel Multimodal Fusion Transformer Architecture with Anti-Overfitting Techniques\n",
        "class MultimodalDrowsinessTransformer(nn.Module):\n",
        "    \"\"\"\n",
        "    Novel multimodal transformer architecture combining:\n",
        "    1. Vision Transformer (ViT) for spatial features\n",
        "    2. Temporal attention for sequence modeling\n",
        "    3. Cross-modal fusion mechanisms\n",
        "    4. Enhanced regularization to prevent overfitting\n",
        "    \"\"\"\n",
        "    def __init__(self, num_classes=7, img_size=224, sequence_length=1, hidden_dim=768):\n",
        "        super().__init__()\n",
        "\n",
        "        # Vision Transformer encoder\n",
        "        self.vit_encoder = ViTModel.from_pretrained('google/vit-base-patch16-224')\n",
        "        self.vit_processor = ViTImageProcessor.from_pretrained('google/vit-base-patch16-224')\n",
        "\n",
        "        # FREEZE FIRST 6 LAYERS OF ViT TO PREVENT OVERFITTING\n",
        "        # Only fine-tune the last layers\n",
        "        num_layers_to_freeze = 6\n",
        "        for i in range(num_layers_to_freeze):\n",
        "            for param in list(self.vit_encoder.encoder.layer[i].parameters()):\n",
        "                param.requires_grad = False\n",
        "\n",
        "        print(f\"‚úì Frozen first {num_layers_to_freeze} ViT encoder layers to prevent overfitting\")\n",
        "\n",
        "        # Feature dimensions\n",
        "        self.vit_dim = hidden_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "\n",
        "        # Multi-head attention for temporal fusion (if sequence_length > 1)\n",
        "        if sequence_length > 1:\n",
        "            self.temporal_attention = nn.MultiheadAttention(hidden_dim, num_heads=8, batch_first=True)\n",
        "            self.temporal_norm = nn.LayerNorm(hidden_dim)\n",
        "\n",
        "        # Cross-modal fusion layers with INCREASED DROPOUT\n",
        "        self.fusion_layer = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.LayerNorm(hidden_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(0.3)  # Increased from 0.1 to 0.3\n",
        "        )\n",
        "\n",
        "        # Classification head with HIGHER DROPOUT RATES\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
        "            nn.LayerNorm(hidden_dim // 2),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(0.5),  # Increased from 0.3 to 0.5\n",
        "            nn.Linear(hidden_dim // 2, hidden_dim // 4),\n",
        "            nn.LayerNorm(hidden_dim // 4),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(0.4),  # Additional dropout layer\n",
        "            nn.Linear(hidden_dim // 4, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, images):\n",
        "        \"\"\"\n",
        "        Forward pass with multimodal fusion\n",
        "        Args:\n",
        "            images: Tensor of shape (batch_size, channels, height, width)\n",
        "        \"\"\"\n",
        "        # Extract vision features using ViT\n",
        "        outputs = self.vit_encoder(pixel_values=images)\n",
        "        vision_features = outputs.last_hidden_state[:, 0, :]  # CLS token\n",
        "\n",
        "        # Apply fusion layer\n",
        "        fused_features = self.fusion_layer(vision_features)\n",
        "\n",
        "        # Classification\n",
        "        logits = self.classifier(fused_features)\n",
        "\n",
        "        return logits, fused_features\n",
        "\n",
        "# Initialize model\n",
        "label_names = sorted(folders)\n",
        "num_classes = len(label_names)\n",
        "model_multimodal = MultimodalDrowsinessTransformer(num_classes=num_classes).to(device)\n",
        "print(f\"\\nModel initialized with {num_classes} classes\")\n",
        "print(f\"Model parameters: {sum(p.numel() for p in model_multimodal.parameters())/1e6:.2f}M\")\n",
        "print(f\"Trainable parameters: {sum(p.numel() for p in model_multimodal.parameters() if p.requires_grad)/1e6:.2f}M\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OwouuYOf64jL"
      },
      "source": [
        "## 5. Dataset Class and Data Loading\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VemeEg6U64jL"
      },
      "source": [
        "## 6. Training with Anti-Overfitting Configuration\n",
        "\n",
        "This section implements comprehensive anti-overfitting measures to ensure robust model performance and prevent overfitting:\n",
        "- **Label Smoothing**: Reduces overconfidence\n",
        "- **Gradient Clipping**: Prevents gradient explosion\n",
        "- **Early Stopping**: Stops training when validation performance plateaus\n",
        "- **Reduced Learning Rate**: More stable training\n",
        "- **Increased Weight Decay**: Stronger regularization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uOrOvg2s64jL"
      },
      "outputs": [],
      "source": [
        "# Custom Dataset Class\n",
        "class DriverDrowsinessDataset(Dataset):\n",
        "    def __init__(self, image_paths, labels, processor, transform=None):\n",
        "        self.image_paths = image_paths\n",
        "        self.labels = labels\n",
        "        self.processor = processor\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.image_paths[idx]\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        # Load and preprocess image\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        # Process with ViT processor\n",
        "        inputs = self.processor(images=image, return_tensors=\"pt\")\n",
        "        pixel_values = inputs['pixel_values'].squeeze(0)\n",
        "\n",
        "        return pixel_values, torch.tensor(label, dtype=torch.long)\n",
        "\n",
        "# Prepare data\n",
        "all_images = []\n",
        "all_labels = []\n",
        "label2id = {name: idx for idx, name in enumerate(sorted(folders))}\n",
        "id2label = {idx: name for name, idx in label2id.items()}\n",
        "\n",
        "for label_name in sorted(folders):\n",
        "    folder_path = os.path.join(BASE_DIR, label_name)\n",
        "    images = [os.path.join(folder_path, f) for f in os.listdir(folder_path)\n",
        "              if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
        "    all_images.extend(images)\n",
        "    all_labels.extend([label2id[label_name]] * len(images))\n",
        "\n",
        "# Train-test split\n",
        "train_images, val_images, train_labels, val_labels = train_test_split(\n",
        "    all_images, all_labels, test_size=0.2, stratify=all_labels, random_state=42\n",
        ")\n",
        "\n",
        "print(f\"Training samples: {len(train_images)}\")\n",
        "print(f\"Validation samples: {len(val_images)}\")\n",
        "\n",
        "# Create datasets\n",
        "processor = ViTImageProcessor.from_pretrained('google/vit-base-patch16-224')\n",
        "train_dataset = DriverDrowsinessDataset(train_images, train_labels, processor)\n",
        "val_dataset = DriverDrowsinessDataset(val_images, val_labels, processor)\n",
        "\n",
        "# Create data loaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=2)\n",
        "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False, num_workers=2)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s7pnaYdL64jM"
      },
      "source": [
        "## 6. Training Loop with Anti-Overfitting Measures\n",
        "\n",
        "**‚ö†Ô∏è IMPORTANT: The actual training code is in Cell 23 below!**\n",
        "\n",
        "This section contains the complete training implementation with all anti-overfitting techniques.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s9fy6e7h64jM"
      },
      "outputs": [],
      "source": [
        "# ====================================================================\n",
        "# üöÄ COMPLETE TRAINING LOOP WITH ANTI-OVERFITTING MEASURES\n",
        "# ====================================================================\n",
        "# This is the MAIN TRAINING CELL - Run this to train your model!\n",
        "# ====================================================================\n",
        "\n",
        "# Training setup with ANTI-OVERFITTING MEASURES\n",
        "print(\"=\"*60)\n",
        "print(\"TRAINING SETUP - ANTI-OVERFITTING CONFIGURATION\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Label smoothing to prevent overfitting (smoothing factor = 0.1)\n",
        "criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
        "print(\"‚úì Label smoothing enabled (smoothing=0.1)\")\n",
        "\n",
        "# Lower learning rate and higher weight decay for better regularization\n",
        "optimizer = torch.optim.AdamW(\n",
        "    model_multimodal.parameters(),\n",
        "    lr=1e-5,  # Reduced from 2e-5 to 1e-5\n",
        "    weight_decay=0.05  # Increased from 0.01 to 0.05\n",
        ")\n",
        "print(\"‚úì Learning rate: 1e-5 (reduced)\")\n",
        "print(\"‚úì Weight decay: 0.05 (increased)\")\n",
        "\n",
        "# Learning rate scheduler (NO verbose parameter - not supported in PyTorch)\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "    optimizer, mode='min', factor=0.5, patience=3\n",
        ")\n",
        "print(\"‚úì Learning rate scheduler enabled (ReduceLROnPlateau)\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Training function with GRADIENT CLIPPING\n",
        "def train_epoch(model, dataloader, criterion, optimizer, device, clip_grad_norm=1.0):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for pixel_values, labels in dataloader:\n",
        "        pixel_values = pixel_values.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        logits, _ = model(pixel_values)\n",
        "        loss = criterion(logits, labels)\n",
        "        loss.backward()\n",
        "\n",
        "        # GRADIENT CLIPPING to prevent exploding gradients and overfitting\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=clip_grad_norm)\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        _, predicted = torch.max(logits.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    return total_loss / len(dataloader), 100 * correct / total\n",
        "\n",
        "# Validation function\n",
        "def validate(model, dataloader, criterion, device):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for pixel_values, labels in dataloader:\n",
        "            pixel_values = pixel_values.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            logits, _ = model(pixel_values)\n",
        "            loss = criterion(logits, labels)\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            _, predicted = torch.max(logits.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "            all_preds.extend(predicted.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    return total_loss / len(dataloader), 100 * correct / total, all_preds, all_labels\n",
        "\n",
        "# ====================================================================\n",
        "# TRAINING LOOP WITH EARLY STOPPING\n",
        "# ====================================================================\n",
        "num_epochs = 8  # Reduced for faster training (early stopping will likely stop before this)\n",
        "best_val_acc = 0\n",
        "best_val_loss = float('inf')\n",
        "patience = 3  # Early stopping patience (reduced for faster training)\n",
        "patience_counter = 0\n",
        "train_losses, val_losses = [], []\n",
        "train_accs, val_accs = [], []\n",
        "best_model_state = None\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"STARTING TRAINING WITH ANTI-OVERFITTING MEASURES\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Max epochs: {num_epochs}\")\n",
        "print(f\"Early stopping patience: {patience} epochs\")\n",
        "print(f\"Gradient clipping: max_norm=1.0\")\n",
        "print(\"=\"*60 + \"\\n\")\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    # Train with gradient clipping\n",
        "    train_loss, train_acc = train_epoch(\n",
        "        model_multimodal, train_loader, criterion, optimizer, device, clip_grad_norm=1.0\n",
        "    )\n",
        "\n",
        "    # Validate\n",
        "    val_loss, val_acc, _, _ = validate(model_multimodal, val_loader, criterion, device)\n",
        "\n",
        "    # Update learning rate\n",
        "    scheduler.step(val_loss)\n",
        "\n",
        "    # Store metrics\n",
        "    train_losses.append(train_loss)\n",
        "    val_losses.append(val_loss)\n",
        "    train_accs.append(train_acc)\n",
        "    val_accs.append(val_acc)\n",
        "\n",
        "    # Print epoch results\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}:\")\n",
        "    print(f\"  Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%\")\n",
        "    print(f\"  Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n",
        "\n",
        "    # Check for improvement\n",
        "    improved = False\n",
        "    if val_acc > best_val_acc:\n",
        "        best_val_acc = val_acc\n",
        "        best_val_loss = val_loss\n",
        "        best_model_state = model_multimodal.state_dict().copy()\n",
        "        torch.save(model_multimodal.state_dict(), 'best_multimodal_transformer.pth')\n",
        "        print(f\"  ‚úì Saved best model (Val Acc: {val_acc:.2f}%)\")\n",
        "        improved = True\n",
        "        patience_counter = 0\n",
        "    elif val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        improved = True\n",
        "        patience_counter = 0\n",
        "    else:\n",
        "        patience_counter += 1\n",
        "\n",
        "    # Check for overfitting warning\n",
        "    if epoch > 0:\n",
        "        if val_loss > val_losses[-2] and train_loss < train_losses[-2]:\n",
        "            print(f\"  ‚ö†Ô∏è  Warning: Possible overfitting (val loss ‚Üë while train loss ‚Üì)\")\n",
        "\n",
        "    # Early stopping\n",
        "    if patience_counter >= patience:\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"EARLY STOPPING triggered after {epoch+1} epochs\")\n",
        "        print(f\"No improvement for {patience} consecutive epochs\")\n",
        "        print(f\"Best validation accuracy: {best_val_acc:.2f}%\")\n",
        "        print(f\"{'='*60}\\n\")\n",
        "        break\n",
        "\n",
        "    print()\n",
        "\n",
        "# Restore best model\n",
        "if best_model_state is not None:\n",
        "    model_multimodal.load_state_dict(best_model_state)\n",
        "    print(f\"{'='*60}\")\n",
        "    print(f\"TRAINING COMPLETED!\")\n",
        "    print(f\"Best validation accuracy: {best_val_acc:.2f}%\")\n",
        "    print(f\"Best model restored and ready for evaluation\")\n",
        "    print(f\"{'='*60}\")\n",
        "else:\n",
        "    print(f\"\\nTraining completed! Best validation accuracy: {best_val_acc:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Qtg5wLx64jN"
      },
      "source": [
        "### ‚ö†Ô∏è Skip to Cell 23 for the Actual Training Code\n",
        "\n",
        "**The training setup and loop with all anti-overfitting measures is in Cell 23.**\n",
        "The cell below (Cell 16) defines the analysis function that will be used AFTER training completes.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JMRWO9SX64jN"
      },
      "source": [
        "## 6.5 Training History Visualization & Overfitting Analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yUWMbYYN64jO"
      },
      "outputs": [],
      "source": [
        "# Visualize training history and detect overfitting for Transformer model\n",
        "def plot_training_history_transformer(train_losses, val_losses, train_accs, val_accs):\n",
        "    \"\"\"Plot training curves and analyze overfitting\"\"\"\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "    epochs = range(1, len(train_accs) + 1)\n",
        "\n",
        "    # Accuracy plot\n",
        "    axes[0].plot(epochs, train_accs, 'o-', label='Train Accuracy', linewidth=2, markersize=8, color='#2E86AB')\n",
        "    axes[0].plot(epochs, val_accs, 's-', label='Val Accuracy', linewidth=2, markersize=8, color='#A23B72')\n",
        "    axes[0].set_title('Transformer Model Accuracy Over Epochs', fontsize=14, fontweight='bold')\n",
        "    axes[0].set_xlabel('Epoch', fontsize=12)\n",
        "    axes[0].set_ylabel('Accuracy (%)', fontsize=12)\n",
        "    axes[0].legend(fontsize=11)\n",
        "    axes[0].grid(alpha=0.3)\n",
        "    axes[0].set_ylim([min(min(train_accs), min(val_accs)) - 2, max(max(train_accs), max(val_accs)) + 2])\n",
        "\n",
        "    # Loss plot\n",
        "    axes[1].plot(epochs, train_losses, 'o-', label='Train Loss', linewidth=2, markersize=8, color='#2E86AB')\n",
        "    axes[1].plot(epochs, val_losses, 's-', label='Val Loss', linewidth=2, markersize=8, color='#A23B72')\n",
        "    axes[1].set_title('Transformer Model Loss Over Epochs', fontsize=14, fontweight='bold')\n",
        "    axes[1].set_xlabel('Epoch', fontsize=12)\n",
        "    axes[1].set_ylabel('Loss', fontsize=12)\n",
        "    axes[1].legend(fontsize=11)\n",
        "    axes[1].grid(alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('training_history_transformer.png', dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "    # Analyze overfitting\n",
        "    print(\"=\"*60)\n",
        "    print(\"OVERFITTING ANALYSIS - MULTIMODAL TRANSFORMER\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # Find best epoch\n",
        "    best_epoch = np.argmax(val_accs) + 1\n",
        "    best_val_acc = max(val_accs)\n",
        "    best_train_acc = train_accs[np.argmax(val_accs)]\n",
        "    best_val_loss = val_losses[np.argmax(val_accs)]\n",
        "    best_train_loss = train_losses[np.argmax(val_accs)]\n",
        "\n",
        "    # Final epoch metrics\n",
        "    final_train_acc = train_accs[-1]\n",
        "    final_val_acc = val_accs[-1]\n",
        "    final_train_loss = train_losses[-1]\n",
        "    final_val_loss = val_losses[-1]\n",
        "\n",
        "    # Calculate gaps\n",
        "    acc_gap_best = best_train_acc - best_val_acc\n",
        "    acc_gap_final = final_train_acc - final_val_acc\n",
        "    loss_gap_best = abs(best_train_loss - best_val_loss)\n",
        "    loss_gap_final = abs(final_train_loss - final_val_loss)\n",
        "\n",
        "    print(f\"Best Model Performance (Epoch {best_epoch}):\")\n",
        "    print(f\"  Train Accuracy: {best_train_acc:.2f}%\")\n",
        "    print(f\"  Val Accuracy:   {best_val_acc:.2f}%\")\n",
        "    print(f\"  Accuracy Gap:   {acc_gap_best:.2f}%\")\n",
        "    print(f\"  Train Loss:     {best_train_loss:.4f}\")\n",
        "    print(f\"  Val Loss:       {best_val_loss:.4f}\")\n",
        "    print(f\"  Loss Gap:       {loss_gap_best:.4f}\")\n",
        "\n",
        "    print(f\"\\nFinal Epoch Performance:\")\n",
        "    print(f\"  Train Accuracy: {final_train_acc:.2f}%\")\n",
        "    print(f\"  Val Accuracy:   {final_val_acc:.2f}%\")\n",
        "    print(f\"  Accuracy Gap:   {acc_gap_final:.2f}%\")\n",
        "    print(f\"  Train Loss:     {final_train_loss:.4f}\")\n",
        "    print(f\"  Val Loss:       {final_val_loss:.4f}\")\n",
        "    print(f\"  Loss Gap:       {loss_gap_final:.4f}\")\n",
        "\n",
        "    # Overfitting indicators\n",
        "    print(f\"\\n\" + \"=\"*60)\n",
        "    print(\"OVERFITTING INDICATORS:\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    overfitting_signs = []\n",
        "\n",
        "    if final_train_acc >= 99.5:\n",
        "        overfitting_signs.append(\"‚ö†Ô∏è  Training accuracy very high (‚â•99.5%) - potential memorization\")\n",
        "\n",
        "    if acc_gap_final > acc_gap_best + 0.5:\n",
        "        overfitting_signs.append(f\"‚ö†Ô∏è  Accuracy gap increased by {abs(acc_gap_final - acc_gap_best):.2f}%\")\n",
        "\n",
        "    if final_val_loss > best_val_loss:\n",
        "        overfitting_signs.append(f\"‚ö†Ô∏è  Validation loss increased from {best_val_loss:.4f} to {final_val_loss:.4f}\")\n",
        "\n",
        "    if final_val_acc < best_val_acc:\n",
        "        overfitting_signs.append(f\"‚ö†Ô∏è  Validation accuracy decreased from {best_val_acc:.2f}% to {final_val_acc:.2f}%\")\n",
        "\n",
        "    # Check if validation loss is increasing while train loss decreasing\n",
        "    if len(val_losses) >= 3:\n",
        "        recent_val_trend = val_losses[-3:]\n",
        "        if recent_val_trend[-1] > recent_val_trend[0] and train_losses[-1] < train_losses[-3]:\n",
        "            overfitting_signs.append(\"‚ö†Ô∏è  Validation loss increasing while training loss decreasing (classic overfitting sign)\")\n",
        "\n",
        "    if len(overfitting_signs) == 0:\n",
        "        print(\"‚úì No significant overfitting detected!\")\n",
        "        print(\"  The model generalizes well to validation data.\")\n",
        "    else:\n",
        "        print(f\"Found {len(overfitting_signs)} indicator(s) of overfitting:\")\n",
        "        for sign in overfitting_signs:\n",
        "            print(f\"  {sign}\")\n",
        "\n",
        "        if len(overfitting_signs) >= 3:\n",
        "            severity = \"SEVERE\"\n",
        "        elif len(overfitting_signs) >= 2:\n",
        "            severity = \"MODERATE\"\n",
        "        else:\n",
        "            severity = \"MILD\"\n",
        "\n",
        "        print(f\"\\nüìä Overfitting Severity: {severity}\")\n",
        "        print(f\"\\nüí° Recommendations for Transformer Model:\")\n",
        "        print(f\"  1. Use the model from Epoch {best_epoch} (best validation performance)\")\n",
        "        print(f\"  2. Increase dropout rates (currently 0.1 in fusion, 0.3 in classifier)\")\n",
        "        print(f\"  3. Add gradient clipping: torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\")\n",
        "        print(f\"  4. Increase weight decay (currently 0.01)\")\n",
        "        print(f\"  5. Add label smoothing to CrossEntropyLoss\")\n",
        "        print(f\"  6. Use mixup or cutmix augmentation\")\n",
        "        print(f\"  7. Reduce learning rate\")\n",
        "        print(f\"  8. Freeze more ViT layers and fine-tune only classifier\")\n",
        "\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    return best_epoch, best_val_acc\n",
        "\n",
        "# Call the analysis function AFTER training completes\n",
        "# Run this cell after training to visualize and analyze overfitting\n",
        "if 'train_losses' in locals() and len(train_losses) > 0:\n",
        "    print(\"=\"*60)\n",
        "    print(\"TRAINING HISTORY ANALYSIS\")\n",
        "    print(\"=\"*60)\n",
        "    best_epoch_analysis, best_val_acc_analysis = plot_training_history_transformer(\n",
        "        train_losses, val_losses, train_accs, val_accs\n",
        "    )\n",
        "    print(f\"\\n‚úì Analysis complete! Best model from Epoch {best_epoch_analysis}\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  Training history not found. Please run the training loop first.\")\n",
        "    print(\"   The analysis will be available after training completes.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D8werSjz64jP"
      },
      "source": [
        "## 4.5 Alternative: Improved Transformer with Anti-Overfitting Techniques\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "79xjCPrc64jP"
      },
      "outputs": [],
      "source": [
        "# OPTIONAL: Improved Transformer model with stronger regularization to reduce overfitting\n",
        "# Uncomment and use this if you want to retrain with anti-overfitting measures\n",
        "\n",
        "\"\"\"\n",
        "class MultimodalDrowsinessTransformerImproved(nn.Module):\n",
        "    \\\"\\\"\\\"\n",
        "    Improved multimodal transformer with anti-overfitting techniques:\n",
        "    - Higher dropout rates\n",
        "    - Label smoothing\n",
        "    - Gradient clipping\n",
        "    - Stronger regularization\n",
        "    \\\"\\\"\\\"\n",
        "    def __init__(self, num_classes=7, img_size=224, sequence_length=1, hidden_dim=768):\n",
        "        super().__init__()\n",
        "\n",
        "        # Vision Transformer encoder (freeze more layers)\n",
        "        self.vit_encoder = ViTModel.from_pretrained('google/vit-base-patch16-224')\n",
        "        # Freeze first 6 layers of ViT\n",
        "        for i in range(6):\n",
        "            for param in list(self.vit_encoder.encoder.layer[i].parameters()):\n",
        "                param.requires_grad = False\n",
        "\n",
        "        self.vit_processor = ViTImageProcessor.from_pretrained('google/vit-base-patch16-224')\n",
        "        self.hidden_dim = hidden_dim\n",
        "\n",
        "        # Cross-modal fusion layers with higher dropout\n",
        "        self.fusion_layer = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.LayerNorm(hidden_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(0.3)  # Increased from 0.1\n",
        "        )\n",
        "\n",
        "        # Classification head with higher dropout\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
        "            nn.LayerNorm(hidden_dim // 2),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(0.5),  # Increased from 0.3\n",
        "            nn.Linear(hidden_dim // 2, hidden_dim // 4),\n",
        "            nn.LayerNorm(hidden_dim // 4),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(0.4),\n",
        "            nn.Linear(hidden_dim // 4, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, images):\n",
        "        outputs = self.vit_encoder(pixel_values=images)\n",
        "        vision_features = outputs.last_hidden_state[:, 0, :]\n",
        "        fused_features = self.fusion_layer(vision_features)\n",
        "        logits = self.classifier(fused_features)\n",
        "        return logits, fused_features\n",
        "\n",
        "# Training with label smoothing\n",
        "criterion_improved = nn.CrossEntropyLoss(label_smoothing=0.1)  # Label smoothing\n",
        "optimizer_improved = torch.optim.AdamW(\n",
        "    model_multimodal_improved.parameters(),\n",
        "    lr=1e-5,  # Lower learning rate\n",
        "    weight_decay=0.05  # Stronger weight decay\n",
        ")\n",
        "\n",
        "# Add gradient clipping in training loop:\n",
        "# torch.nn.utils.clip_grad_norm_(model_multimodal_improved.parameters(), max_norm=1.0)\n",
        "\"\"\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HvftGoUL64jP"
      },
      "source": [
        "## 6.6 Load Best Model (Non-Overfit Version)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Dtflr8a64jQ"
      },
      "source": [
        "## 6.7 Training History Visualization & Overfitting Analysis\n",
        "\n",
        "**Run this cell after training completes** to visualize training curves and analyze overfitting patterns.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H1mTXvvr64jQ"
      },
      "source": [
        "## 6. Training Setup & Loop with Anti-Overfitting\n",
        "\n",
        "**This cell contains the complete training loop with all anti-overfitting measures:**\n",
        "- ‚úÖ Label smoothing (0.1)\n",
        "- ‚úÖ Reduced learning rate (1e-5)  \n",
        "- ‚úÖ Higher weight decay (0.05)\n",
        "- ‚úÖ Gradient clipping (max_norm=1.0)\n",
        "- ‚úÖ Early stopping (patience=5)\n",
        "- ‚úÖ Learning rate scheduling\n",
        "- ‚úÖ Overfitting detection warnings\n",
        "\n",
        "**Run this cell to start training!**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aF3wMMm564jQ"
      },
      "outputs": [],
      "source": [
        "# Training completed - now analyze the results\n",
        "# Check if training has been completed\n",
        "if 'train_losses' in locals() and len(train_losses) > 0:\n",
        "    print(\"=\"*60)\n",
        "    print(\"TRAINING HISTORY ANALYSIS\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # Visualize and analyze training history\n",
        "    best_epoch_analysis, best_val_acc_analysis = plot_training_history_transformer(\n",
        "        train_losses, val_losses, train_accs, val_accs\n",
        "    )\n",
        "\n",
        "    print(f\"\\n‚úì Analysis complete! Best model from Epoch {best_epoch_analysis}\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  Training history not found. Please run the training loop first (Cell 19).\")\n",
        "    print(\"   The analysis will be available after training completes.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nZwoPhqX64jQ"
      },
      "outputs": [],
      "source": [
        "**Note:** This cell was removed as it was a duplicate of the training loop in Section 6. Please use Cell 15 for training.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b0bMtedv64jQ"
      },
      "source": [
        "## 7. Model Evaluation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c_8vHX6J64jQ"
      },
      "outputs": [],
      "source": [
        "# Load best model and evaluate\n",
        "model_multimodal.load_state_dict(torch.load('best_multimodal_transformer.pth'))\n",
        "val_loss, val_acc, y_pred, y_true = validate(model_multimodal, val_loader, criterion, device)\n",
        "\n",
        "# Classification report\n",
        "print(\"=\"*60)\n",
        "print(\"CLASSIFICATION REPORT\")\n",
        "print(\"=\"*60)\n",
        "print(classification_report(y_true, y_pred, target_names=sorted(folders)))\n",
        "\n",
        "# Confusion matrix\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=sorted(folders), yticklabels=sorted(folders))\n",
        "plt.title('Confusion Matrix - Multimodal Transformer', fontsize=14, fontweight='bold')\n",
        "plt.ylabel('True Label')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.tight_layout()\n",
        "plt.savefig('confusion_matrix_transformer.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "# Binary drowsiness evaluation\n",
        "drowsy_labels = ['Yawn', 'closed']\n",
        "drowsy_ids = [label2id[label] for label in drowsy_labels if label in label2id]\n",
        "\n",
        "y_true_bin = [1 if id2label[y] in drowsy_labels else 0 for y in y_true]\n",
        "y_pred_bin = [1 if id2label[y] in drowsy_labels else 0 for y in y_pred]\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"BINARY DROWSINESS CLASSIFICATION REPORT\")\n",
        "print(\"=\"*60)\n",
        "print(classification_report(y_true_bin, y_pred_bin, target_names=['Not Drowsy', 'Drowsy']))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jPLyMDcq64jQ"
      },
      "source": [
        "## 8. Interactive Dashboard with Gradio\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D7MXWmgd64jQ"
      },
      "outputs": [],
      "source": [
        "# Install Gradio\n",
        "%pip install gradio -q\n",
        "\n",
        "import gradio as gr\n",
        "from PIL import Image\n",
        "\n",
        "def predict_image_multimodal(img_path):\n",
        "    \"\"\"Predict using multimodal transformer\"\"\"\n",
        "    if img_path is None:\n",
        "        return None, None, None\n",
        "\n",
        "    model_multimodal.eval()\n",
        "    processor = ViTImageProcessor.from_pretrained('google/vit-base-patch16-224')\n",
        "\n",
        "    # Load and preprocess image\n",
        "    image = Image.open(img_path).convert('RGB')\n",
        "    inputs = processor(images=image, return_tensors=\"pt\")\n",
        "    pixel_values = inputs['pixel_values'].to(device)\n",
        "\n",
        "    # Predict\n",
        "    with torch.no_grad():\n",
        "        logits, _ = model_multimodal(pixel_values)\n",
        "        probs = torch.nn.functional.softmax(logits, dim=-1)\n",
        "        pred_id = torch.argmax(probs, dim=-1).item()\n",
        "        pred_class = id2label[pred_id]\n",
        "        confidence = probs[0][pred_id].item()\n",
        "\n",
        "    # Binary drowsiness status\n",
        "    drowsy_labels = ['Yawn', 'closed']\n",
        "    binary_status = \"Drowsy\" if pred_class in drowsy_labels else \"Not Drowsy\"\n",
        "\n",
        "    # Class probabilities\n",
        "    class_probs = {id2label[i]: float(probs[0][i]) for i in range(len(id2label))}\n",
        "\n",
        "    output_text = f\"**Predicted Class:** {pred_class}\\\\n\"\n",
        "    output_text += f\"**Confidence:** {confidence*100:.2f}%\\\\n\"\n",
        "    output_text += f\"**Drowsiness Status:** {binary_status}\"\n",
        "\n",
        "    return output_text, pred_class, class_probs\n",
        "\n",
        "# Create Gradio interface\n",
        "iface = gr.Interface(\n",
        "    fn=predict_image_multimodal,\n",
        "    inputs=gr.Image(type=\"filepath\"),\n",
        "    outputs=[\n",
        "        gr.Markdown(label=\"Prediction Results\"),\n",
        "        gr.Textbox(label=\"Predicted Class\"),\n",
        "        gr.Label(label=\"Class Probabilities\")\n",
        "    ],\n",
        "    title=\"üöó Driver Drowsiness Detection - Multimodal Transformer\",\n",
        "    description=\"Upload an image to predict driver drowsiness using our novel multimodal transformer architecture.\"\n",
        ")\n",
        "\n",
        "iface.launch(share=False, server_name=\"0.0.0.0\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
